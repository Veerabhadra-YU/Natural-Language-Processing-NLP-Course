{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veerabhadra-YU/Natural-Language-Processing-NLP-Course/blob/main/Assignment_4_Transformer_part_III_Veerabhadra_Rao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III\n",
        "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach.\n",
        "\n",
        "Please create a transformer-based classifier for English name classification into male or female.\n",
        "\n",
        "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications.\n",
        "\n",
        "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment.\n",
        "\n",
        "```\n",
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "['female.txt', 'male.txt']\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "[w for w in male_names if w in female_names]\n",
        "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
        "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
        "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
        "```"
      ],
      "metadata": {
        "id": "QD5ia2HsZpsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKQa86RpY5rH"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "None\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_datasets\n",
        "!pip install -U tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHYJuy3VTPd-",
        "outputId": "53813e3d-cfa9-41c1-e38f-491f88713fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.25.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.66.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.5.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.11.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2024.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow_datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.63.0)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkZ8rK1iUV-g",
        "outputId": "6fdc693b-a78b-40f3-dcdb-1f07afd60b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder-Decoder Approach (Transformer-based)**"
      ],
      "metadata": {
        "id": "4WB3tDStvb2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load data\n",
        "male_names = ['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis', 'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel', 'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil']\n",
        "female_names = ['Alice', 'Alicia', 'Alina', 'Alison', 'Alissa', 'Allyson', 'Alma', 'Althea', 'Alva', 'Alyson', 'Alyssa', 'Amber', 'Amelia', 'Amie', 'Amy', 'Ana', 'Anastasia', 'Andrea', 'Angel', 'Angela', 'Angelia', 'Angelica', 'Angelina', 'Angeline']\n",
        "\n",
        "# Create dataset\n",
        "all_names = male_names + female_names\n",
        "labels = [0] * len(male_names) + [1] * len(female_names)\n",
        "\n",
        "class NamesDataset(Dataset):\n",
        "    def __init__(self, names, labels, tokenizer, max_length):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(name, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Split data\n",
        "train_names, val_names, train_labels, val_labels = train_test_split(all_names, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = NamesDataset(train_names, train_labels, tokenizer, max_length=10)\n",
        "val_dataset = NamesDataset(val_names, val_labels, tokenizer, max_length=10)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Define model\n",
        "class NameClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size, num_classes):\n",
        "        super(NameClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=45):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = correct_preds / total_preds\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = correct_preds / total_preds\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "model = NameClassifier(hidden_size=768, num_classes=2)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5L5SWvaS-q9",
        "outputId": "271c21a8-9ae0-4729-cbd4-29a0f5482587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/45:\n",
            "Train Loss: 0.6932, Train Acc: 0.5385, Val Loss: 0.7733, Val Acc: 0.3000\n",
            "Epoch 2/45:\n",
            "Train Loss: 0.7034, Train Acc: 0.5641, Val Loss: 0.7077, Val Acc: 0.3000\n",
            "Epoch 3/45:\n",
            "Train Loss: 0.6698, Train Acc: 0.6923, Val Loss: 0.7169, Val Acc: 0.3000\n",
            "Epoch 4/45:\n",
            "Train Loss: 0.6104, Train Acc: 0.6410, Val Loss: 0.7224, Val Acc: 0.3000\n",
            "Epoch 5/45:\n",
            "Train Loss: 0.5726, Train Acc: 0.7436, Val Loss: 0.7524, Val Acc: 0.2000\n",
            "Epoch 6/45:\n",
            "Train Loss: 0.5195, Train Acc: 0.7949, Val Loss: 0.8112, Val Acc: 0.3000\n",
            "Epoch 7/45:\n",
            "Train Loss: 0.4686, Train Acc: 0.8974, Val Loss: 0.7106, Val Acc: 0.5000\n",
            "Epoch 8/45:\n",
            "Train Loss: 0.4451, Train Acc: 0.8718, Val Loss: 0.7138, Val Acc: 0.6000\n",
            "Epoch 9/45:\n",
            "Train Loss: 0.3465, Train Acc: 0.9487, Val Loss: 0.7145, Val Acc: 0.6000\n",
            "Epoch 10/45:\n",
            "Train Loss: 0.2724, Train Acc: 0.9487, Val Loss: 0.7048, Val Acc: 0.5000\n",
            "Epoch 11/45:\n",
            "Train Loss: 0.2028, Train Acc: 0.9744, Val Loss: 0.7565, Val Acc: 0.7000\n",
            "Epoch 12/45:\n",
            "Train Loss: 0.2274, Train Acc: 0.9487, Val Loss: 0.8058, Val Acc: 0.7000\n",
            "Epoch 13/45:\n",
            "Train Loss: 0.1572, Train Acc: 0.9487, Val Loss: 0.8799, Val Acc: 0.3000\n",
            "Epoch 14/45:\n",
            "Train Loss: 0.1208, Train Acc: 0.9744, Val Loss: 1.0706, Val Acc: 0.3000\n",
            "Epoch 15/45:\n",
            "Train Loss: 0.1244, Train Acc: 0.9487, Val Loss: 0.9183, Val Acc: 0.4000\n",
            "Epoch 16/45:\n",
            "Train Loss: 0.0817, Train Acc: 1.0000, Val Loss: 0.9643, Val Acc: 0.4000\n",
            "Epoch 17/45:\n",
            "Train Loss: 0.0958, Train Acc: 0.9487, Val Loss: 1.0891, Val Acc: 0.4000\n",
            "Epoch 18/45:\n",
            "Train Loss: 0.0708, Train Acc: 0.9744, Val Loss: 1.0257, Val Acc: 0.5000\n",
            "Epoch 19/45:\n",
            "Train Loss: 0.0840, Train Acc: 0.9744, Val Loss: 0.8909, Val Acc: 0.5000\n",
            "Epoch 20/45:\n",
            "Train Loss: 0.0588, Train Acc: 0.9744, Val Loss: 1.1041, Val Acc: 0.6000\n",
            "Epoch 21/45:\n",
            "Train Loss: 0.0600, Train Acc: 0.9744, Val Loss: 1.1906, Val Acc: 0.6000\n",
            "Epoch 22/45:\n",
            "Train Loss: 0.0646, Train Acc: 0.9487, Val Loss: 1.0893, Val Acc: 0.6000\n",
            "Epoch 23/45:\n",
            "Train Loss: 0.0499, Train Acc: 0.9744, Val Loss: 0.9138, Val Acc: 0.6000\n",
            "Epoch 24/45:\n",
            "Train Loss: 0.0511, Train Acc: 0.9744, Val Loss: 1.1089, Val Acc: 0.5000\n",
            "Epoch 25/45:\n",
            "Train Loss: 0.0456, Train Acc: 0.9744, Val Loss: 1.3642, Val Acc: 0.4000\n",
            "Epoch 26/45:\n",
            "Train Loss: 0.0655, Train Acc: 0.9744, Val Loss: 1.5364, Val Acc: 0.3000\n",
            "Epoch 27/45:\n",
            "Train Loss: 0.0768, Train Acc: 0.9487, Val Loss: 2.0827, Val Acc: 0.2000\n",
            "Epoch 28/45:\n",
            "Train Loss: 0.1153, Train Acc: 0.9231, Val Loss: 1.6790, Val Acc: 0.6000\n",
            "Epoch 29/45:\n",
            "Train Loss: 0.1415, Train Acc: 0.9487, Val Loss: 1.5293, Val Acc: 0.7000\n",
            "Epoch 30/45:\n",
            "Train Loss: 0.0497, Train Acc: 1.0000, Val Loss: 1.2466, Val Acc: 0.5000\n",
            "Epoch 31/45:\n",
            "Train Loss: 0.0625, Train Acc: 0.9487, Val Loss: 1.2093, Val Acc: 0.5000\n",
            "Epoch 32/45:\n",
            "Train Loss: 0.0701, Train Acc: 0.9744, Val Loss: 1.3646, Val Acc: 0.5000\n",
            "Epoch 33/45:\n",
            "Train Loss: 0.0699, Train Acc: 0.9744, Val Loss: 1.3668, Val Acc: 0.5000\n",
            "Epoch 34/45:\n",
            "Train Loss: 0.0641, Train Acc: 0.9744, Val Loss: 1.5411, Val Acc: 0.7000\n",
            "Epoch 35/45:\n",
            "Train Loss: 0.0666, Train Acc: 0.9744, Val Loss: 1.5949, Val Acc: 0.7000\n",
            "Epoch 36/45:\n",
            "Train Loss: 0.0725, Train Acc: 0.9744, Val Loss: 1.6163, Val Acc: 0.7000\n",
            "Epoch 37/45:\n",
            "Train Loss: 0.0807, Train Acc: 0.9231, Val Loss: 1.2119, Val Acc: 0.5000\n",
            "Epoch 38/45:\n",
            "Train Loss: 0.0461, Train Acc: 0.9744, Val Loss: 0.9583, Val Acc: 0.6000\n",
            "Epoch 39/45:\n",
            "Train Loss: 0.0882, Train Acc: 0.9744, Val Loss: 1.0485, Val Acc: 0.5000\n",
            "Epoch 40/45:\n",
            "Train Loss: 0.0461, Train Acc: 0.9744, Val Loss: 1.0443, Val Acc: 0.5000\n",
            "Epoch 41/45:\n",
            "Train Loss: 0.0444, Train Acc: 1.0000, Val Loss: 1.0238, Val Acc: 0.6000\n",
            "Epoch 42/45:\n",
            "Train Loss: 0.0352, Train Acc: 0.9744, Val Loss: 0.9929, Val Acc: 0.6000\n",
            "Epoch 43/45:\n",
            "Train Loss: 0.0547, Train Acc: 0.9487, Val Loss: 0.9912, Val Acc: 0.6000\n",
            "Epoch 44/45:\n",
            "Train Loss: 0.0451, Train Acc: 0.9744, Val Loss: 0.9891, Val Acc: 0.6000\n",
            "Epoch 45/45:\n",
            "Train Loss: 0.0443, Train Acc: 0.9744, Val Loss: 0.9989, Val Acc: 0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_names(model, tokenizer, names):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    for name in names:\n",
        "        encoding = tokenizer(name, padding='max_length', truncation=True, max_length=10, return_tensors='pt')\n",
        "        input_ids = encoding['input_ids'].flatten().unsqueeze(0)\n",
        "        attention_mask = encoding['attention_mask'].flatten().unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.append(predicted.item())\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Test the model on the test set\n",
        "test_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "# Get predicted labels for test names\n",
        "test_names = val_dataset.names\n",
        "predicted_labels = predict_names(model, tokenizer, test_names)\n",
        "\n",
        "# Map predicted labels to class names\n",
        "class_names = ['male', 'female']\n",
        "predicted_classes = [class_names[label] for label in predicted_labels]\n",
        "\n",
        "# Print names along with predicted classes\n",
        "for name, predicted_class in zip(test_names, predicted_classes):\n",
        "    print(f'{name}: {predicted_class}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKAaRhccsBV",
        "outputId": "ea37dfcd-8fd9-46cb-876b-12d0188444a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.5881, Test Acc: 0.7000\n",
            "Allyn: female\n",
            "Angelia: female\n",
            "Angelina: female\n",
            "Angela: female\n",
            "Angel: female\n",
            "Alina: female\n",
            "Alicia: female\n",
            "Alice: male\n",
            "Alma: female\n",
            "Ariel: male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/Assignment_4_Transformer_part_III_Veerabhadra_Rao.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVxDxKNgxrCO",
        "outputId": "d98acc6d-8bfe-4d02-e496-1a28ea14c6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/Assignment_4_Transformer_part_III_Veerabhadra_Rao.ipynb to html\n",
            "[NbConvertApp] Writing 634670 bytes to /content/Assignment_4_Transformer_part_III_Veerabhadra_Rao.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. https://arxiv.org/pdf/2102.03692.pdf\n",
        "2. https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/13-attention.html\n",
        "3. https://towardsdatascience.com/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044\n",
        "4. https://www.nltk.org/book/ch02.html#sec-lexical-resources"
      ],
      "metadata": {
        "id": "ExMITGgCdQWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHiDsdXLhbbW"
      }
    }
  ]
}